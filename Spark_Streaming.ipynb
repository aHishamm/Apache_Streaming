{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "import pyspark.sql.types as T \n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#data from https://www.kaggle.com/datasets/ealaxi/paysim1\n",
    "df = spark.read.csv(\"PS_20174392719_1491204439457_log.csv\",header=True,inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step',\n",
       " 'type',\n",
       " 'amount',\n",
       " 'nameOrig',\n",
       " 'oldbalanceOrg',\n",
       " 'newbalanceOrig',\n",
       " 'nameDest',\n",
       " 'oldbalanceDest',\n",
       " 'newbalanceDest',\n",
       " 'isFraud',\n",
       " 'isFlaggedFraud']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"isFraud\",\"isFlaggedFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|\n",
      "|   1| PAYMENT| 7817.71|  C90045638|      53860.0|      46042.29| M573487274|           0.0|           0.0|\n",
      "|   1| PAYMENT| 7107.77| C154988899|     183195.0|     176087.23| M408069119|           0.0|           0.0|\n",
      "|   1| PAYMENT| 7861.64|C1912850431|    176087.23|     168225.59| M633326333|           0.0|           0.0|\n",
      "|   1| PAYMENT| 4024.36|C1265012928|       2671.0|           0.0|M1176932104|           0.0|           0.0|\n",
      "|   1|   DEBIT| 5337.77| C712410124|      41720.0|      36382.23| C195600860|       41898.0|      40348.79|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "|  31|   12|\n",
      "|  34|30904|\n",
      "|  28|    4|\n",
      "|  26|  440|\n",
      "|  27|   41|\n",
      "|  12|36153|\n",
      "|  22|12635|\n",
      "|   1| 2708|\n",
      "|  13|37515|\n",
      "|   6| 1660|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Each step is a unit of time mapped to the real world \n",
    "df.groupBy(\"step\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data \n",
    "!mkdir data/chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#saving the output by filtering on each step and saving it to a separate file\n",
    "steps = df.select(\"step\").distinct().collect() \n",
    "for step in steps[:]: \n",
    "    _df = df.where(f\"step = {step[0]}\") \n",
    "    _df.coalesce(1).write.mode(\"append\").option(\"header\",\"true\").csv(\"data/chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the files \n",
    "!cd data/chunks/ && ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one part of the streaming data \n",
    "subset = spark.read.csv(\n",
    "    \"data/chunks/part-00000-00cc0439-322d-4a05-acae-90429fa2b352-c000.csv\",\n",
    "    header = True, \n",
    "    inferSchema = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "|  39|23391|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset.groupBy(\"step\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('step', IntegerType(), True), StructField('type', StringType(), True), StructField('amount', DoubleType(), True), StructField('nameOrig', StringType(), True), StructField('oldbalanceOrg', DoubleType(), True), StructField('newbalanceOrig', DoubleType(), True), StructField('nameDest', StringType(), True), StructField('oldbalanceDest', DoubleType(), True), StructField('newbalanceDest', DoubleType(), True)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#streaming version of this subset \n",
    "schema = subset.schema\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to adjust this to view only one file per trigger \n",
    "streaming = (\n",
    "    spark.readStream.schema(schema).option(\"maxFilesPerTrigger\",1).csv(\"data/chunks/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I will setup the transformation \n",
    "# nameDest column is the recipient ID of the transaction \n",
    "dest_count = streaming.groupBy(\"nameDest\").count().orderBy(F.desc(\"count\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 186:===========================>                         (103 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "activityQuery = (\n",
    "    dest_count.writeStream.queryName(\"dest_counts\").format(\"memory\").outputMode(\"complete\").start()\n",
    ")\n",
    "\n",
    "import time \n",
    "for x in range(50): \n",
    "    _df = spark.sql(\"SELECT * FROM dest_counts WHERE nameDest != 'nameDest' AND count>= 2\")\n",
    "    if _df.count() > 0: \n",
    "        _df.show(10) \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 194:=================>                                    (63 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "spark.streams.active[0].isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:==================================>                  (129 + 8) / 200]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:=====================================>               (140 + 8) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:===============================================>     (180 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activityQuery.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
